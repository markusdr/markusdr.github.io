[{"authors":["admin"],"categories":null,"content":"Markus Dreyer is a Principal Applied Scientist at Amazon, where he works on large language models, text generation, and multi-agent systems. He contributed to the Amazon Nova family of foundation models and led the development of Nova Deep Research. His research spans summarization, question answering, and natural language understanding, and he has published at venues including ACL, EMNLP, NAACL, and NeurIPS. He holds a Ph.D. in Computer Science from Johns Hopkins University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Markus Dreyer is a Principal Applied Scientist at Amazon, where he works on large language models, text generation, and multi-agent systems. He contributed to the Amazon Nova family of foundation models and led the development of Nova Deep Research. His research spans summarization, question answering, and natural language understanding, and he has published at venues including ACL, EMNLP, NAACL, and NeurIPS. He holds a Ph.D. in Computer Science from Johns Hopkins University.","tags":null,"title":"Markus Dreyer","type":"authors"},{"authors":["Prahaladh Chandrahasan","Jiahe Jin","Zhihan Zhang","Tevin Wang","Andy Tang","Lucy Mo","Morteza Ziyadi","Leonardo FR Ribeiro","Zimeng Qiu","Markus Dreyer"," others"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704293,"objectID":"22b888c036499c579cfda7103bed8e6b","permalink":"/publication/chandrahasan-2025-deep/","publishdate":"2026-02-21T20:04:53.592366Z","relpermalink":"/publication/chandrahasan-2025-deep/","section":"publication","summary":"Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation.","tags":["large language models","multi-agent systems"],"title":"Deep Research Comparator: A Platform for Fine-Grained Human Annotations of Deep Research Agents","type":"publication"},{"authors":["Markus Dreyer","Can Liu","Sujith Ravi"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704697,"objectID":"8b3ea1b956e83c5a27d226147d26dee1","permalink":"/publication/dreyer-2025-meaning/","publishdate":"2026-02-21T20:11:37.602661Z","relpermalink":"/publication/dreyer-2025-meaning/","section":"publication","summary":"","tags":["summarization"],"title":"Meaning Summarization Techniques","type":"publication"},{"authors":["Max Glockner","Xiang Jiang","Leonardo FR Ribeiro","Iryna Gurevych","Markus Dreyer"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704293,"objectID":"3e08d7b88340d3b0be7f9e8309ddf642","permalink":"/publication/glockner-2025-neoqa/","publishdate":"2026-02-21T20:04:53.099993Z","relpermalink":"/publication/glockner-2025-neoqa/","section":"publication","summary":"Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\u0026A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.","tags":["question answering"],"title":"NeoQA: Evidence-Based Question Answering with Generated News Events","type":"publication"},{"authors":["Aashiq Muhamed","Leonardo FR Ribeiro","Markus Dreyer","Virginia Smith","Mona T Diab"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704294,"objectID":"9cec669eee6840c00f4ac0a4c9e289ea","permalink":"/publication/muhamed-2025-refusalbench/","publishdate":"2026-02-21T20:04:53.82708Z","relpermalink":"/publication/muhamed-2025-refusalbench/","section":"publication","summary":"The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting either dangerous overconfidence or overcaution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarks -- RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) -- and our complete generation framework to enable continued, dynamic evaluation of this critical capability.","tags":["large language models","evaluation"],"title":"RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models","type":"publication"},{"authors":["Aaron Langford","Aayush Shah","Abhanshu Gupta","Abhimanyu Bhatter","Abhinav Goyal","Abhinav Mathur","Abhinav Mohanty","Abhishek Kumar","Abhishek Sethi","Abi Komma"," others"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704293,"objectID":"ae3cb82a998d1d878023fa51161a4574","permalink":"/publication/langford-2025-amazon/","publishdate":"2026-02-21T20:04:53.374055Z","relpermalink":"/publication/langford-2025-amazon/","section":"publication","summary":"We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.","tags":["large language models"],"title":"The Amazon Nova Family of Models: Technical Report and Model Card","type":"publication"},{"authors":["Xiang Jiang","Markus Dreyer"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704292,"objectID":"edfcddf715ee110e120387d98a25a0a4","permalink":"/publication/jiang-2024-ccsum/","publishdate":"2026-02-21T20:04:52.674358Z","relpermalink":"/publication/jiang-2024-ccsum/","section":"publication","summary":"Training a supervised news summarization model requires large amounts of high-quality training data consisting of news articles paired with reference summaries. However, obtaining such data is costly, and existing datasets contain considerable amount of noise. We present a new large-scale and high-quality dataset for supervised abstractive news summarization containing 1.3 million training samples, which we call CCSum. In creating this dataset, we take advantage of the journalistic inverted-pyramid style in news writing: In some articles, the first sentence can be considered a summary of the reported story. Accordingly, among 35 million CommonCrawl News articles, we identify pairs of articles about the same news story and use one article's first sentence as the summary for the other article. To ensure high quality, we apply strict filters whose parameters we optimize using Bayesian optimization. We show that the resulting dataset is more factual and informative than established summarization datasets; less than 1% of the summaries have major factual inconsistencies with the corresponding news articles, compared to 5.5% to 15.4% in existing datasets, according to our human evaluation. Summarization models trained on our dataset are more favored compared to those trained on CNN/Daily Mail. The proposed dataset can open new opportunities for future research in abstractive summarization.","tags":["summarization"],"title":"CCSum: A Large-Scale and High-Quality Dataset for Abstractive News Summarization","type":"publication"},{"authors":["Alyssa Hwang","Kalpit Dixit","Miguel Ballesteros","Yassine Benajiba","Vittorio Castelli","Markus Dreyer","Mohit Bansal","Kathleen McKeown"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704292,"objectID":"fdd6e998d2fe4373a8ba87ff43c1d7d3","permalink":"/publication/hwang-2024-newsqs/","publishdate":"2026-02-21T20:04:52.223079Z","relpermalink":"/publication/hwang-2024-newsqs/","section":"publication","summary":"","tags":["question answering"],"title":"NewsQs: Multi-Source Question Generation for the Inquiring Mind","type":"publication"},{"authors":["Vaidehi Patil","Leonardo FR Ribeiro","Mengwen Liu","Mohit Bansal","Markus Dreyer"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704293,"objectID":"2319e2f232ed8b063200b88ab5c71a35","permalink":"/publication/patil-2024-refinesumm/","publishdate":"2026-02-21T20:04:52.88766Z","relpermalink":"/publication/patil-2024-refinesumm/","section":"publication","summary":"Multimodal Large Language Models (MLLMs) excel at synthesizing key information from diverse sources. However, generating accurate and faithful multimodal summaries is challenging, primarily due to the lack of appropriate multimodal datasets for fine-tuning that meaningfully integrate textual and visual modalities. To address this gap, we present a new dataset designed specifically for image-text multimodal summarization, harnessing the capabilities of state-of-the-art MLLMs. We generate summaries from Wikipedia sections and corresponding images and evaluate them across text-based, visual and multimodal dimensions, employing reference-free metrics. To refine the dataset, we: (1) Filter the MLLM-generated summaries by training a critic model on human annotations and using its predictions to remove low-quality summaries; (2) Fine-tune the MLLM with the filtered high-quality summaries; (3) Use the fine-tuned model in turn to regenerate the summaries. This self-refinement process significantly improves summary quality, as measured by human judgements and automatic multimodal metrics, resulting in a valuable dataset for multimodal summarization research.","tags":["summarization","multimodal"],"title":"RefineSumm: Self-Refining MLLM for Generating a Multimodal Summarization Dataset","type":"publication"},{"authors":["Ziyang Wang","Heba Elfardy","Markus Dreyer","Kevin Small","Mohit Bansal"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704292,"objectID":"472e6600fc0339b532afeb54ef889f11","permalink":"/publication/wang-2024-unified/","publishdate":"2026-02-21T20:04:52.452544Z","relpermalink":"/publication/wang-2024-unified/","section":"publication","summary":"","tags":["multimodal","large language models"],"title":"Unified Embeddings for Multimodal Retrieval via Frozen LLMs","type":"publication"},{"authors":["Zixuan Zhang","Heba Elfardy","Markus Dreyer","Kevin Small","Heng Ji","Mohit Bansal"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874053,"objectID":"15e4691c22c552f709a0b669f9fd1b6b","permalink":"/publication/zhang-etal-2023-enhancing/","publishdate":"2023-11-25T01:06:19.410971Z","relpermalink":"/publication/zhang-etal-2023-enhancing/","section":"publication","summary":"Information extraction (IE) and summarization are closely related, both tasked with presenting a subset of the information contained in a natural language text. However, while IE extracts structural representations, summarization aims to abstract the most salient information into a generated text summary -- thus potentially encountering the technical limitations of current text generation methods (e.g., hallucination). To mitigate this risk, this work uses structured IE graphs to enhance the abstractive summarization task. Specifically, we focus on improving Multi-Document Summarization (MDS) performance by using cross-document IE output, incorporating two novel components: (1) the use of auxiliary entity and event recognition systems to focus the summary generation model; (2) incorporating an alignment loss between IE nodes and their text spans to reduce inconsistencies between the IE graphs and text representations. Operationally, both the IE nodes and corresponding text spans are projected into the same embedding space and pairwise distance is minimized. Experimental results on multiple MDS benchmarks show that summaries generated by our model are more factually consistent with the source documents than baseline models while maintaining the same level of abstractiveness.","tags":["summarization"],"title":"Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction","type":"publication"},{"authors":["Markus Dreyer","Mengwen Liu","Feng Nan","Sandeep Atluri","Sujith Ravi"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874053,"objectID":"bbfbb4e3bb4f9247dd7c6a37f32a4d0d","permalink":"/publication/dreyer-etal-2023-evaluating/","publishdate":"2023-11-25T01:06:19.766032Z","relpermalink":"/publication/dreyer-etal-2023-evaluating/","section":"publication","summary":"Neural models for abstractive summarization tend to generate output that is fluent and well-formed but lacks semantic faithfulness, or factuality, with respect to the input documents. In this paper, we analyze the tradeoff between abstractiveness and factuality of generated summaries across multiple datasets and models, using extensive human evaluations of factuality. In our analysis, we visualize the rates of change in factuality as we gradually increase abstractiveness using a decoding constraint, and we observe that, while increased abstractiveness generally leads to a drop in factuality, the rate of factuality decay depends on factors such as the data that the system was trained on. We introduce two datasets with human factuality judgements; one containing 10.2k generated summaries with systematically varied degrees of abstractiveness; the other containing 4.2k summaries from five different summarization models. We propose new factuality metrics that adjust for the degree of abstractiveness, and we use them to compare the abstractiveness-adjusted factuality of previous summarization works, providing baselines for future work.","tags":["summarization","factuality"],"title":"Evaluating the Tradeoff Between Abstractiveness and Factuality in Abstractive Summarization","type":"publication"},{"authors":["David Wan","Mengwen Liu","Kathleen McKeown","Markus Dreyer","Mohit Bansal"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874052,"objectID":"d6f138445af1967c7baa19a3b489183b","permalink":"/publication/wan-etal-2023-faithfulness/","publishdate":"2023-11-25T01:06:19.055799Z","relpermalink":"/publication/wan-etal-2023-faithfulness/","section":"publication","summary":"Despite significant progress in understanding and improving faithfulness in abstractive summarization, the question of how decoding strategies affect faithfulness is less studied. We present a systematic study of the effect of generation techniques such as beam search and nucleus sampling on faithfulness in abstractive summarization. We find a consistent trend where beam search with large beam sizes produces the most faithful summaries while nucleus sampling generates the least faithful ones. We propose two faithfulness-aware generation methods to further improve faithfulness over current generation techniques: (1) ranking candidates generated by beam search using automatic faithfulness metrics and (2) incorporating lookahead heuristics that produce a faithfulness score on the future summary. We show that both generation methods significantly improve faithfulness across two datasets as evaluated by four automatic faithfulness metrics and human evaluation. To reduce computational cost, we demonstrate a simple distillation approach that allows the model to generate faithful summaries with just greedy decoding.","tags":["summarization","factuality"],"title":"Faithfulness-Aware Decoding Strategies for Abstractive Summarization","type":"publication"},{"authors":["Adithya Pratapa","Kevin Small","Markus Dreyer"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874381,"objectID":"d6282bb6f573ceeaf1798380742bb822","permalink":"/publication/pratapa-2023-background/","publishdate":"2023-11-25T01:06:20.822201Z","relpermalink":"/publication/pratapa-2023-background/","section":"publication","summary":"Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experiments show the effectiveness of instruction fine-tuned systems such as Flan-T5, in addition to strong zero-shot performance using GPT-3.5.","tags":["summarization"],"title":"Background Summarization of Event Timelines","type":"publication"},{"authors":["Leonardo F. R. Ribeiro","Mohit Bansal","Markus Dreyer"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874380,"objectID":"760f5f3a23b6a3dd16fbba43b4c619de","permalink":"/publication/ribeiro-2023-generating/","publishdate":"2023-11-25T01:06:20.471307Z","relpermalink":"/publication/ribeiro-2023-generating/","section":"publication","summary":"Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader's background knowledge. Generating summaries based on different readability levels is critical for enabling knowledge consumption by diverse audiences. However, current text generation approaches lack refined control, resulting in texts that are not customized to readers' proficiency levels. In this work, we bridge this gap and study techniques to generate summaries at specified readability levels. Unlike previous methods that focus on a specific readability level (e.g., lay summarization), we generate summaries with fine-grained control over their readability. We develop three text generation techniques for controlling readability: (1) instruction-based readability control, (2) reinforcement learning to minimize the gap between requested and observed readability and (3) a decoding approach that uses lookahead to estimate the readability of upcoming decoding steps. We show that our generation methods significantly improve readability control on news summarization (CNN/DM dataset), as measured by various readability metrics and human judgement, establishing strong baselines for controllable readability in summarization.","tags":["summarization"],"title":"Generating Summaries with Controllable Readability Levels","type":"publication"},{"authors":["Jonathan Pilault","Can Liu","Mohit Bansal","Markus Dreyer"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874380,"objectID":"76d11805a6eb6adb29686a5caeea5328","permalink":"/publication/dblp-confijcai-pilault-lbd-23/","publishdate":"2023-11-25T01:06:20.12009Z","relpermalink":"/publication/dblp-confijcai-pilault-lbd-23/","section":"publication","summary":"Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine-tuned models, on compositional generalization tasks, controllable summarization and multilingual translation, while needing fewer trainable parameters.","tags":["large language models"],"title":"On Conditional and Compositional Language Model Differentiable Prompting","type":"publication"},{"authors":["Arthur Brazinskas","Ramesh Nallapati","Mohit Bansal","Markus Dreyer"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874052,"objectID":"d387fe0fa35306228953d3d48beaa0a1","permalink":"/publication/brazinskas-etal-2022-efficient/","publishdate":"2023-11-25T01:06:18.699478Z","relpermalink":"/publication/brazinskas-etal-2022-efficient/","section":"publication","summary":"Abstractive summarization models are typically pre-trained on large amounts of generic texts, then fine-tuned on tens or hundreds of thousands of annotated samples. However, in opinion summarization, large annotated datasets of reviews paired with reference summaries are not available and would be expensive to create. This calls for fine-tuning methods robust to overfitting on small datasets. In addition, generically pre-trained models are often not accustomed to the specifics of customer reviews and, after fine-tuning, yield summaries with disfluencies and semantic mistakes. To address these problems, we utilize an efficient few-shot method based on adapters which, as we show, can easily store in-domain knowledge. Instead of fine-tuning the entire model, we add adapters and pre-train them in a task-specific way on a large corpus of unannotated customer reviews, using held-out reviews as pseudo summaries. Then, fine-tune the adapters on the small available human-annotated dataset. We show that this self-supervised adapter pre-training improves summary quality over standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp datasets, respectively. Finally, for summary personalization, we condition on aspect keyword queries, automatically created from generic datasets. In the same vein, we pre-train the adapters in a query-based manner on customer reviews and then fine-tune them on annotated datasets. This results in better-organized summary content reflected in improved coherence and fewer redundancies.","tags":["summarization"],"title":"Efficient Few-Shot Fine-Tuning for Opinion Summarization","type":"publication"},{"authors":["Leonardo F. R. Ribeiro","Mengwen Liu","Iryna Gurevych","Markus Dreyer","Mohit Bansal"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700874052,"objectID":"7b5b0bbb4d131333c2057d2a4a80fbcf","permalink":"/publication/ribeiro-etal-2022-factgraph/","publishdate":"2023-11-25T01:06:18.343945Z","relpermalink":"/publication/ribeiro-etal-2022-factgraph/","section":"publication","summary":"Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.","tags":["summarization","factuality"],"title":"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations","type":"publication"},{"authors":["Markus Dreyer","Mengwen Liu","Feng Nan","Sandeep Atluri","Sujith Ravi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640709377,"objectID":"ae96d08cd8d1b2d9684900d00efb0aef","permalink":"/publication/dblp-journalscorrabs-2108-02859/","publishdate":"2023-11-25T01:06:17.995314Z","relpermalink":"/publication/dblp-journalscorrabs-2108-02859/","section":"publication","summary":"We analyze the tradeoff between factuality and abstractiveness of summaries. We introduce abstractiveness constraints to control the degree of abstractiveness at decoding time, and we apply this technique to characterize the abstractiveness-factuality tradeoff across multiple widely-studied datasets, using extensive human evaluations. We train a neural summarization model on each dataset and visualize the rates of change in factuality as we gradually increase abstractiveness using our abstractiveness constraints. We observe that, while factuality generally drops with increased abstractiveness, different datasets lead to different rates of factuality decay. We propose new measures to quantify the tradeoff between factuality and abstractiveness, incl. muQAGS, which balances factuality with abstractiveness. We also quantify this tradeoff in previous works, aiming to establish baselines for the abstractiveness-factuality tradeoff that future publications can compare against.","tags":["summarization","factuality"],"title":"Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints","type":"publication"},{"authors":["Ramakanth Pasunuru","Mengwen Liu","Mohit Bansal","Sujith Ravi","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640652060,"objectID":"d27c285c589c0afcb7fd8a9a8056c22c","permalink":"/publication/pasunuru-2021-efficiently-st/","publishdate":"2023-11-25T01:06:17.292937Z","relpermalink":"/publication/pasunuru-2021-efficiently-st/","section":"publication","summary":"This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.","tags":["summarization"],"title":"Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters","type":"publication"},{"authors":["Khalil Mrini","Can Liu","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640652060,"objectID":"b0a5f945f2f2d9ed032c23cee0d3c77f","permalink":"/publication/mrini-2021-rewards-wn/","publishdate":"2023-11-25T01:06:17.643728Z","relpermalink":"/publication/mrini-2021-rewards-wn/","section":"publication","summary":"We consider the problem of topic-focused abstractive summarization, where the goal is to generate an abstractive summary focused on a particular topic, a phrase of one or multiple words. We hypothesize that the task of generating topic-focused summaries can be improved by showing the model what it must not focus on. We introduce a deep reinforcement learning approach to topic-focused abstractive summarization, trained on rewards with a novel negative example baseline. We define the input in this problem as the source text preceded by the topic. We adapt the CNN-Daily Mail and New York Times summarization datasets for this task. We then show through experiments on existing rewards that the use of a negative example baseline can outperform the use of a self-critical baseline, in Rouge, BERTScore, and human evaluation metrics.","tags":["summarization","reinforcement learning"],"title":"Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization","type":"publication"},{"authors":["Arthur Brazinskas","Mengwen Liu","Ramesh Nallapati","Sujith Ravi","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640709539,"objectID":"1770f5bc6eee08116a3f5f3748046abf","permalink":"/publication/dblp-journalscorrabs-2104-09500/","publishdate":"2023-11-25T00:48:10.421405Z","relpermalink":"/publication/dblp-journalscorrabs-2104-09500/","section":"publication","summary":"Pre-trained language models have recently advanced abstractive summarization. These models are further fine-tuned on human-written references before summary generation in test time. In this work, we propose the first application of transductive learning to summarization. In this paradigm, a model can learn from the test set's input before inference. To perform transduction, we propose to utilize input document summarizing sentences to construct references for learning in test time. These sentences are often compressed and fused to form abstractive summaries and provide omitted details and additional context to the reader. We show that our approach yields state-of-the-art results on CNN/DM and NYT datasets. For instance, we achieve over 1 ROUGE-L point improvement on CNN/DM. Further, we show the benefits of transduction from older to more recent news. Finally, through human and automatic evaluation, we show that our summaries become more abstractive and coherent.","tags":["summarization"],"title":"Transductive Learning for Abstractive News Summarization","type":"publication"},{"authors":["Boya Yu","Avani Deshpande","Adrian Mark McLeod","Naga Sai Likhitha Patha","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704697,"objectID":"9da729d1d7c30b8b5bbd9e1635e0c2bc","permalink":"/publication/yu-2021-word/","publishdate":"2026-02-21T20:11:37.102143Z","relpermalink":"/publication/yu-2021-word/","section":"publication","summary":"","tags":["natural language processing"],"title":"Word Embeddings for Natural Language Processing","type":"publication"},{"authors":["William Clinton Dabney","Arpit Gupta","Faisal Ladhak","Markus Dreyer","Anjishnu Kumar"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704697,"objectID":"751acfacfe955e31ad24965fb010a8e9","permalink":"/publication/dabney-2020-voice/","publishdate":"2026-02-21T20:11:36.889506Z","relpermalink":"/publication/dabney-2020-voice/","section":"publication","summary":"","tags":["natural language understanding"],"title":"Voice User Interface Knowledge Acquisition System","type":"publication"},{"authors":["Shiva Pentyala","Mengwen Liu","Markus Dreyer"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"dfc82e4090d689cc6ec6def8d2d93ead","permalink":"/publication/pentyala-etal-2019-multi/","publishdate":"2023-11-25T01:06:16.255754Z","relpermalink":"/publication/pentyala-etal-2019-multi/","section":"publication","summary":"We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.","tags":["natural language understanding","transfer learning"],"title":"Multi-Task Networks with Universe, Group, and Task Feature Learning","type":"publication"},{"authors":["Markus Dreyer","Pavankumar Reddy Muddireddy","Anjishnu Kumar"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704696,"objectID":"360cfe527243a980a2bdcb40df2bd450","permalink":"/publication/dreyer-2019-extendable/","publishdate":"2026-02-21T20:11:36.663894Z","relpermalink":"/publication/dreyer-2019-extendable/","section":"publication","summary":"","tags":["natural language understanding"],"title":"Extendable Label Recognition of Linguistic Input","type":"publication"},{"authors":["Faisal Ladhak","Ankur Gandhe","Markus Dreyer","Ariya Rastrow","Björn Hoffmeister","Lambert Mathias"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704696,"objectID":"095bc904449b5f59c25c3fbb6b61733c","permalink":"/publication/ladhak-2019-lattice-2/","publishdate":"2026-02-21T20:11:36.431547Z","relpermalink":"/publication/ladhak-2019-lattice-2/","section":"publication","summary":"","tags":["speech","deep learning"],"title":"Lattice Decoding and Result Confirmation Using Recurrent Neural Networks","type":"publication"},{"authors":["Faisal Ladhak","Ankur Gandhe","Markus Dreyer","Ariya Rastrow","Björn Hoffmeister","Lambert Mathias"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704696,"objectID":"6bacf068360db9bc370571ffe425443c","permalink":"/publication/ladhak-2019-lattice-1/","publishdate":"2026-02-21T20:11:35.83607Z","relpermalink":"/publication/ladhak-2019-lattice-1/","section":"publication","summary":"","tags":["speech","deep learning"],"title":"Lattice Encoding Using Recurrent Neural Networks","type":"publication"},{"authors":["Daniel Marcu","Markus Dreyer"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704695,"objectID":"4289961a92368542467f2432f1f92dc8","permalink":"/publication/marcu-2019-method-1/","publishdate":"2026-02-21T20:11:35.134378Z","relpermalink":"/publication/marcu-2019-method-1/","section":"publication","summary":"","tags":["machine translation"],"title":"Method and System for Automatic Management of Reputation of Translators","type":"publication"},{"authors":["Daniel Marcu","Markus Dreyer"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704696,"objectID":"ba7aff843969b16ad9fe25378a0240ce","permalink":"/publication/marcu-2019-method-2/","publishdate":"2026-02-21T20:11:36.08837Z","relpermalink":"/publication/marcu-2019-method-2/","section":"publication","summary":"","tags":["machine translation"],"title":"Method and System for Automatic Management of Reputation of Translators","type":"publication"},{"authors":["Anjishnu Kumar","Markus Dreyer"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704695,"objectID":"5db90d6a6c3398a8a1ef914e5dc31aef","permalink":"/publication/kumar-2018-neural/","publishdate":"2026-02-21T20:11:35.357189Z","relpermalink":"/publication/kumar-2018-neural/","section":"publication","summary":"","tags":["natural language understanding","deep learning"],"title":"Neural Latent Variable Model for Spoken Language Understanding","type":"publication"},{"authors":["Ariya Rastrow","Nikko Ström","Spyridon Matsoukas","Markus Dreyer","Ankur Gandhe","Denis Sergeyevich Filimonov","Julian Chan","Rohit Prasad"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1771704695,"objectID":"16c4794a38cd82aad8815f0c54a9f85b","permalink":"/publication/rastrow-2018-speech/","publishdate":"2026-02-21T20:11:35.575328Z","relpermalink":"/publication/rastrow-2018-speech/","section":"publication","summary":"","tags":["speech","natural language understanding"],"title":"Speech Processing with Learned Representation of User Interaction History","type":"publication"},{"authors":["Anjishnu Kumar","Arpit Gupta","Julian Chan","Sam Tucker","Bjorn Hoffmeister","Markus Dreyer","Stanislav Peshterliev","Ankur Gandhe","Denis Filiminov","Ariya Rastrow","Christian Monson","Agnika Kumar"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"f53388c5ddfd06f49b163e07085e6c8c","permalink":"/publication/kumar-2017-just-ask/","publishdate":"2023-11-25T01:06:13.455553Z","relpermalink":"/publication/kumar-2017-just-ask/","section":"publication","summary":"This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK) a large scale Spoken Language Understanding (SLU) Software Development Kit (SDK) that enables developers to extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the infrastructure powers over 25,000 skills deployed through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability and a rapid iteration cycle for third party developers. It imposes inductive biases that allow it to learn robust SLU models from extremely small and sparse datasets and, in doing so, removes significant barriers to entry for software developers and dialogue systems researchers.","tags":["natural language understanding","speech"],"title":"Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding","type":"publication"},{"authors":["Xing Fan","Emilio Monti","Lambert Mathias","Markus Dreyer"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"93f94c96c49937888b94ed2faa335a70","permalink":"/publication/fan-etal-2017-transfer/","publishdate":"2023-11-25T01:06:14.501187Z","relpermalink":"/publication/fan-etal-2017-transfer/","section":"publication","summary":"The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to the target task with smaller labeled data. We see an absolute accuracy gain ranging from 1.0% to 4.4% in in our in-house data set and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.","tags":["semantic parsing","transfer learning"],"title":"Transfer Learning for Neural Semantic Parsing","type":"publication"},{"authors":["Anjishnu Kumar","Pavankumar Reddy Muddireddy","Markus Dreyer","Björn Hoffmeister"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bb741db2b0256e4dd2c60d6f199cf41a","permalink":"/publication/kumar-2017/","publishdate":"2023-11-25T01:06:15.902649Z","relpermalink":"/publication/kumar-2017/","section":"publication","summary":"We present a zero-shot learning approach for text classification, predicting which natural language understanding domain can handle a given utterance. Our approach can predict domains at runtime that did not exist at training time. We achieve this extensibility by learning to project utterances and domains into the same embedding space while generating each domain-specific embedding from a set of attributes that characterize the domain. Our model is a neural network trained via ranking loss. We evaluate the performance of this zero-shot approach on a subset of a virtual assistant’s third-party domains and show the effectiveness of the technique on new domains not observed during training. We compare to generative baselines and show that our approach requires less storage and performs better on new domains.","tags":["natural language understanding","transfer learning"],"title":"Zero-Shot Learning Across Heterogeneous Overlapping Domains","type":"publication"},{"authors":["Faisal Ladhak","Ankur Gandhe","Markus Dreyer","Lambert Mathias","Ariya Rastrow","Björn Hoffmeister"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0ce7d3de6671c831e8da6762400f2a47","permalink":"/publication/ladhak-2016-lattice-rnn-rn/","publishdate":"2023-11-25T01:06:14.15148Z","relpermalink":"/publication/ladhak-2016-lattice-rnn-rn/","section":"publication","summary":"We present a new model called LatticeRnn, which generalizes recurrent neural networks (RNNs) to process weighted lattices as input, instead of sequences. A LatticeRnn can encode the complete structure of a lattice into a dense representation, which makes it suitable to a variety of problems, including rescoring, classifying, parsing, or translating lattices using deep neural networks (DNNs). In this paper, we use LatticeRnns for a classification task: each lattice represents the output from an automatic speech recognition (ASR) component of a spoken language understanding (SLU) system, and we classify the intent of the spoken utterance based on the lattice embedding computed by a LatticeRnn. We show that making decisions based on the full ASR output lattice, as opposed to 1-best or n-best hypotheses, makes SLU systems more robust to ASR errors. Our experiments yield improvements of 13% over a baseline RNN system trained on transcriptions and 10% over an n-best list rescoring system for intent classification.","tags":["speech","deep learning"],"title":"LatticeRnn: Recurrent Neural Networks Over Lattices","type":"publication"},{"authors":["Markus Dreyer","Jonathan Graehl"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"3326e50931ec0a64aa523f4d23839497","permalink":"/publication/dreyer-graehl-2015-hyp/","publishdate":"2023-11-25T01:06:16.600709Z","relpermalink":"/publication/dreyer-graehl-2015-hyp/","section":"publication","summary":"We present hyp, an open-source toolkit for the representation, manipulation, and optimization of weighted directed hypergraphs. hyp provides compose, project, invert functionality, k-best path algorithms, the inside and outside algorithms, and more. Finite-state machines are modeled as a special case of directed hypergraphs. hyp consists of a C++ API, as well as a command line tool, and is available for download at github.com/sdl-research/hyp.","tags":["machine translation"],"title":"hyp: A Toolkit for Representing, Manipulating, and Optimizing Hypergraphs","type":"publication"},{"authors":["Markus Dreyer","Yuanzhe Dong"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"d3488d285b9d4db10cac69f3616a8b2f","permalink":"/publication/dreyer-dong-2015-apro/","publishdate":"2023-11-25T01:06:16.946964Z","relpermalink":"/publication/dreyer-dong-2015-apro/","section":"publication","summary":"We present APRO, a new method for machine translation tuning that can handle large feature sets. As opposed to other popular methods (e.g., MERT, MIRA, PRO), which involve randomness and require multiple runs to obtain a reliable result, APRO gives the same result on any run, given initial feature weights. APRO follows the pairwise ranking approach of PRO (Hopkins and May, 2011), but instead of ranking a small sampled subset of pairs from the kbest list, APRO efficiently ranks all pairs. By obviating the need for manually determined sampling settings, we obtain more reliable results. APRO converges more quickly than PRO and gives similar or better translation results.","tags":["machine translation"],"title":"APRO: All-Pairs Ranking Optimization for MT Tuning","type":"publication"},{"authors":["Markus Dreyer","Daniel Marcu"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"f6763c75d648b3ea653017fc59fd07bc","permalink":"/publication/dreyer-marcu-2012-hyter/","publishdate":"2023-11-25T01:06:12.401224Z","relpermalink":"/publication/dreyer-marcu-2012-hyter/","section":"publication","summary":"It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.","tags":["machine translation","evaluation"],"title":"HyTER: Meaning-Equivalent Semantics for Translation Evaluation","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"db5525d0094de1fa351ff07d75941755","permalink":"/publication/dreyer-eisner-2011/","publishdate":"2023-11-25T01:06:10.274703Z","relpermalink":"/publication/dreyer-eisner-2011/","section":"publication","summary":"We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming.  Given 50–100 seed paradigms, adding a 10-million-word corpus reduces prediction error for morphological inflections by up to 10%.","tags":["morphology","graphical models"],"title":"Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model","type":"publication"},{"authors":["Markus Dreyer"],"categories":null,"content":"","date":1301616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301616000,"objectID":"b684e655afd4ff8e8493383be27d0368","permalink":"/publication/dreyer-2011/","publishdate":"2023-11-25T01:06:10.638408Z","relpermalink":"/publication/dreyer-2011/","section":"publication","summary":"The field of statistical natural language processing has been turning toward morphologically rich languages. These languages have vocabularies that are often orders of magnitude larger than that of English, since words may be inflected in various different ways. This leads to problems with data sparseness and calls for models that can deal with this abundance of related words—models that can learn, analyze, reduce and generate morphological inflections. But surprisingly, statistical approaches to morphology are still rare, which stands in contrast to the many recent advances of sophisticated models in parsing, grammar induction, translation and many other areas of natural language processing. This thesis presents a novel, unified statistical approach to inflectional morphology, an approach that can decode and encode the inflectional system of a language. At the center of this approach stands the notion of inflectional paradigms. These paradigms cluster the large vocabulary of a language into structured chunks; inflections of the same word, like break, broke, breaks, breaking, … , all belong in the same paradigm. And moreover, each of these inflections has an exact place within a paradigm, since each paradigm has designated slots for each possible inflection; for verbs, there is a slot for the first person singular indicative present, one for the third person plural subjunctive past and slots for all other possible forms. The main goal of this thesis is to build probability models over inflectional paradigms, and therefore to sort the large vocabulary of a morphologically rich language into structured clusters. These models can be learned with minimal supervision for any language that has inflectional morphology. As training data, some sample paradigms and a raw, unannotated text corpus can be used. The models over morphological paradigms are developed in three main chapters that start with smaller components and build up to larger ones. The first of these chapters (Chapter 2) presents novel probability models over strings and string pairs. These are applicable to lemmatization or to relate a past tense form to its associated present tense form, or for similar morphological tasks. It turns out they are general enough to tackle the popular task of transliteration very well, as well as other string-to-string tasks. The second (Chapter 3) introduces the notion of a probability model over multiple strings, which is a novel variant of Markov Random Fields. These are used to relate the many inflections in an inflectional paradigm to one another, and they use the probability models from Chapter 2 as components. A novel version of belief propagation is presented, which propagates distributions over strings through a network of connected finite-state transducers, to perform inference in morphological paradigms (or other string fields). Finally (Chapter 4), a non-parametric joint probability model over an unannotated text corpus and the morphological paradigms from Chapter 3 is presented. This model is based on a generative story for inflectional morphology that naturally incorporates common linguistic notions, such as lexemes, paradigms and inflections. Sampling algorithms are presented that perform inference over large text corpora and their implicit, hidden morphological paradigms. We show that they are able to discover the morphological paradigms that are implicit in the corpora. The model is based on finite-state operations and seamlessly handles concatenative and nonconcatenative morphology.","tags":["morphology","graphical models"],"title":"A Non-Parametric Model for the Discovery of Inflectional Paradigms from Plain Text Using Graphical Models over Strings","type":"publication"},{"authors":["Ariya Rastrow","Markus Dreyer","Abhinav Sethy","Sanjeev Khudanpur","Bhuvana Ramabhadran","Mark Dredze"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"aced1d049c1d17d93718ddd5d847d4ea","permalink":"/publication/conficassp-rastrow-dskrd-11/","publishdate":"2023-11-25T01:06:15.20312Z","relpermalink":"/publication/conficassp-rastrow-dskrd-11/","section":"publication","summary":"We describe a new approach for rescoring speech lattices - with long-span language models or wide-context acoustic models - that does not entail computationally intensive lattice expansion or limited rescoring of only an N-best list. We view the set of word-sequences in a lattice as a discrete space equipped with the edit-distance metric, and develop a hill climbing technique to start with, say, the 1-best hypothesis under the lattice-generating model(s) and iteratively search a local neighborhood for the highest-scoring hypothesis under the rescoring model(s); such neighborhoods are efficiently constructed via finite state techniques. We demonstrate empirically that to achieve the same reduction in error rate using a better estimated, higher order language model, our technique evaluates fewer utterance-length hypotheses than conventional N-best rescoring by two orders of magnitude. For the same number of hypotheses evaluated, our technique results in a significantly lower error rate.","tags":["speech"],"title":"Hill climbing on speech lattices: A new rescoring framework","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1249084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1249084800,"objectID":"5cf13edf8c132bf15e8c47cecc64842a","permalink":"/publication/dreyer-eisner-2009/","publishdate":"2023-11-25T01:06:10.99777Z","relpermalink":"/publication/dreyer-eisner-2009/","section":"publication","summary":"We study graphical modeling in the case of string-valued random variables. Whereas a weighted finite-state transducer can model the probabilistic relationship between two strings, we are interested in building up joint models of three or more strings. This is needed for inflectional paradigms in morphology, cognate modeling or language reconstruction, and multiple-string alignment. We propose a Markov Random Field in which each factor (potential function) is a weighted finite-state machine, typically a transducer that evaluates the relationship between just two of the strings. The full joint distribution is then a product of these factors. Though decoding is actually undecidable in general, we can still do efficient joint inference using approximate belief propagation; the necessary computations and messages are all finite-state. We demonstrate the methods by jointly predicting morphological forms.","tags":["graphical models","morphology"],"title":"Graphical Models over Multiple Strings","type":"publication"},{"authors":["Paul McNamee","Mark Dredze","Adam Gerber","Nikesh Garera","Tim Finin","James Mayfield","Christine Piatko","Delip Rao","David Yarowsky","Markus Dreyer"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"ac966ed6ec6f6d03d5fcc5d017ca5b0e","permalink":"/publication/mc-namee-2009/","publishdate":"2023-11-25T01:06:12.756219Z","relpermalink":"/publication/mc-namee-2009/","section":"publication","summary":"The HLTCOE participated in the entity linking and slot filling tasks at TAC 2009. A machine learning-based approach to entity linking, operating over a wide range of feature types, yielded good performance on the entity linking task. Slot-filling based on sentence selection, application of weak patterns and exploitation of redundancy was ineffective in the slot filling task.","tags":["information extraction"],"title":"HLTCOE Approaches to Knowledge Base Population at TAC 2009","type":"publication"},{"authors":["Markus Dreyer","Jason R. Smith","Jason Eisner"],"categories":null,"content":"","date":1222819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222819200,"objectID":"7b3f3a69acd442072e8b28556b5c90e4","permalink":"/publication/dreyer-smith-eisner-2008/","publishdate":"2023-11-25T01:06:11.348833Z","relpermalink":"/publication/dreyer-smith-eisner-2008/","section":"publication","summary":"String-to-string transduction is a central problem in computational linguistics and natural language processing. It occurs in tasks as diverse as name transliteration, spelling correction, pronunciation modeling and inflectional morphology. We present a conditional loglinear model for string-to-string transduction, which employs overlapping features over latent alignment sequences, and which learns latent classes and latent string pair regions from incomplete training data. We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results, even when trained on small data sets. On the task of generating morphological forms, we outperform a baseline method reducing the error rate by up to 48%. On a lemmatization task, we reduce the error rates in Wicentowski (2002) by 38–92%.","tags":["morphology","graphical models"],"title":"Latent-Variable Modeling of String Transductions with Finite-State Methods","type":"publication"},{"authors":["Damianos Karakos","Jason Eisner","Sanjeev Khudanpur","Markus Dreyer"],"categories":null,"content":"","date":1212278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212278400,"objectID":"e314034706499c2d068995210fac7cef","permalink":"/publication/karakos-et-al-2008/","publishdate":"2023-11-25T01:06:11.700682Z","relpermalink":"/publication/karakos-et-al-2008/","section":"publication","summary":"Given several systems' automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks.","tags":["machine translation"],"title":"Machine Translation System Combination using ITG-based Alignments","type":"publication"},{"authors":["Markus Dreyer","Keith Hall","Sanjeev Khudanpur"],"categories":null,"content":"","date":1175385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1175385600,"objectID":"85fbc607e0885299b5f92994a909c4f7","permalink":"/publication/dreyer-etal-2007-comparing/","publishdate":"2023-11-25T01:06:13.100336Z","relpermalink":"/publication/dreyer-etal-2007-comparing/","section":"publication","summary":"This paper describes a new method to compare reordering constraints for Statistical Machine Translation. We investigate the best possible (oracle) BLEU score achievable under different reordering constraints. Using dynamic programming, we efficiently find a reordering that approximates the highest attainable BLEU score given a reference and a set of reordering constraints. We present an empirical evaluation of popular reordering constraints: local constraints, the IBM constraints, and the Inversion Transduction Grammar (ITG) constraints. We present results for a German-English translation task and show that reordering under the ITG constraints can improve over the baseline by more than 7.5 BLEU points.","tags":["machine translation"],"title":"Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation","type":"publication"},{"authors":["M. Dreyer","I. Shafran"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"6438ca4ac574feab4c9536023c829f12","permalink":"/publication/dreyer-2007/","publishdate":"2023-11-25T01:06:15.550112Z","relpermalink":"/publication/dreyer-2007/","section":"publication","summary":"We propose novel methods for integrating prosody in syntax using generative models. By adopting a grammar whose constituents have latent annotations, the influence of prosody on syntax can be learned from data. In one method, prosody is utilized to seed the latent annotations of a grammar which is then refined using EM iterations. In an orthogonal approach, we integrate prosody into grammar more explicitly using a model that jointly observes words and associated prosody. We evaluate the two methods by parsing speech data from the Switchboard corpus. The results are compared against baseline results from a model that does not use prosody. The experiments show that prosody improves a grammar in terms of accuracy as well as the parsimonious use of parameters.","tags":["parsing","speech"],"title":"Exploiting prosody for PCFGs with latent annotations","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1151712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1151712000,"objectID":"665c5b751f24130443ca0bed6ac82f35","permalink":"/publication/dreyer-eisner-2006/","publishdate":"2023-11-25T01:06:12.051041Z","relpermalink":"/publication/dreyer-eisner-2006/","section":"publication","summary":"We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.","tags":["parsing"],"title":"Better Informed Training of Latent Syntactic Features","type":"publication"},{"authors":["Markus Dreyer","David A. Smith","Noah A. Smith"],"categories":null,"content":"","date":1149120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1149120000,"objectID":"3476c4de032824b8e25cbdaa25604242","permalink":"/publication/dreyer-etal-2006-vine/","publishdate":"2023-11-25T01:06:13.807042Z","relpermalink":"/publication/dreyer-etal-2006-vine/","section":"publication","summary":"We describe our entry in the CoNLL-X shared task. The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of crosslingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data).","tags":["parsing"],"title":"Vine Parsing and Minimum Risk Reranking for Speed and Precision","type":"publication"},{"authors":["A. Burbank","M. Carpuat","S. Clark","M. Dreyer","P. Fox","D. Groves","K. Hall","M. Hearne","I. D Melamed","Y. Shen"," Others"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"f01dcac7b84df73a4977102e03a3ec4c","permalink":"/publication/burbank-2005/","publishdate":"2023-11-25T01:06:14.852078Z","relpermalink":"/publication/burbank-2005/","section":"publication","summary":"Designers of SMT system have begun to experiment with tree-structured translation models. Unfortunately, SMT systems driven by such models are even more difficult to build than the already complicated WFST-based systems. The purpose of our workshop was to lower the barriers to entry into research involving such SMT systems. Our goals were inspired by the successful 1999 MT workshop, which had a similar purpose. Specifically, we wanted to follow that precedent to 1. build a publicly available toolkit for experimenting with tree-structured translation models; 2. build a tool for visualizing the predictions of such models; 3. demonstrate the feasibility of SMT with tree-structured models by running baseline experiments on large datasets; and 4. demonstrate that it is easy to retarget the toolkit to new language pairs.","tags":["machine translation","parsing"],"title":"Final report of the 2005 language engineering workshop on statistical machine translation by parsing","type":"publication"}]