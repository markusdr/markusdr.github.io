[{"authors":["admin"],"categories":null,"content":"Markus Dreyer is a machine learning scientist at Amazon. He has been part of the Alexa group since 2015, leading projects in natural language understanding and question answering. He has published on transfer learning, semi-supervised learning, graphical models, semantic parsing, neural lattice parsing, and entity linking.\n","date":1461110400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Markus Dreyer is a machine learning scientist at Amazon. He has been part of the Alexa group since 2015, leading projects in natural language understanding and question answering. He has published on transfer learning, semi-supervised learning, graphical models, semantic parsing, neural lattice parsing, and entity linking.","tags":null,"title":"Markus Dreyer","type":"authors"},{"authors":["Markus Dreyer","Mengwen Liu","Feng Nan","Sandeep Atluri","Sujith Ravi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640709377,"objectID":"ae96d08cd8d1b2d9684900d00efb0aef","permalink":"/publication/dblp-journalscorrabs-2108-02859/","publishdate":"2021-12-28T16:36:17.203096Z","relpermalink":"/publication/dblp-journalscorrabs-2108-02859/","section":"publication","summary":"We analyze the tradeoff between factuality and abstractiveness of summaries. We introduce abstractiveness constraints to control the degree of abstractiveness at decoding time, and we apply this technique to characterize the abstractiveness-factuality tradeoff across multiple widely-studied datasets, using extensive human evaluations. We train a neural summarization model on each dataset and visualize the rates of change in factuality as we gradually increase abstractiveness using our abstractiveness constraints. We observe that, while factuality generally drops with increased abstractiveness, different datasets lead to different rates of factuality decay. We propose new measures to quantify the tradeoff between factuality and abstractiveness, incl. muQAGS, which balances factuality with abstractiveness. We also quantify this tradeoff in previous works, aiming to establish baselines for the abstractiveness-factuality tradeoff that future publications can compare against.","tags":[],"title":"Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints","type":"publication"},{"authors":["Ramakanth Pasunuru","Mengwen Liu","Mohit Bansal","Sujith Ravi","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640652060,"objectID":"d27c285c589c0afcb7fd8a9a8056c22c","permalink":"/publication/pasunuru-2021-efficiently-st/","publishdate":"2021-12-28T00:55:44.661761Z","relpermalink":"/publication/pasunuru-2021-efficiently-st/","section":"publication","summary":"This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.","tags":[],"title":"Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters","type":"publication"},{"authors":["Khalil Mrini","Can Liu","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640652060,"objectID":"b0a5f945f2f2d9ed032c23cee0d3c77f","permalink":"/publication/mrini-2021-rewards-wn/","publishdate":"2021-12-28T00:55:44.846287Z","relpermalink":"/publication/mrini-2021-rewards-wn/","section":"publication","summary":"We consider the problem of topic-focused abstractive summarization, where the goal is to generate an abstractive summary focused on a particular topic, a phrase of one or multiple words. We hypothesize that the task of generating topic-focused summaries can be improved by showing the model what it must not focus on. We introduce a deep reinforcement learning approach to topic-focused abstractive summarization, trained on rewards with a novel negative example baseline. We define the input in this problem as the source text preceded by the topic. We adapt the CNN-Daily Mail and New York Times summarization datasets for this task. We then show through experiments on existing rewards that the use of a negative example baseline can outperform the use of a self-critical baseline, in Rouge, BERTScore, and human evaluation metrics.","tags":[],"title":"Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization","type":"publication"},{"authors":["Arthur Brazinskas","Mengwen Liu","Ramesh Nallapati","Sujith Ravi","Markus Dreyer"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640709539,"objectID":"1770f5bc6eee08116a3f5f3748046abf","permalink":"/publication/dblp-journalscorrabs-2104-09500/","publishdate":"2021-12-28T16:38:59.051239Z","relpermalink":"/publication/dblp-journalscorrabs-2104-09500/","section":"publication","summary":"Pre-trained language models have recently advanced abstractive summarization. These models are further fine-tuned on human-written references before summary generation in test time. In this work, we propose the first application of transductive learning to summarization. In this paradigm, a model can learn from the test set's input before inference. To perform transduction, we propose to utilize input document summarizing sentences to construct references for learning in test time. These sentences are often compressed and fused to form abstractive summaries and provide omitted details and additional context to the reader. We show that our approach yields state-of-the-art results on CNN/DM and NYT datasets. For instance, we achieve over 1 ROUGE-L point improvement on CNN/DM. Further, we show the benefits of transduction from older to more recent news. Finally, through human and automatic evaluation, we show that our summaries become more abstractive and coherent.","tags":[],"title":"Transductive Learning for Abstractive News Summarization","type":"publication"},{"authors":["Shiva Pentyala","Mengwen Liu","Markus Dreyer"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"dfc82e4090d689cc6ec6def8d2d93ead","permalink":"/publication/pentyala-etal-2019-multi/","publishdate":"2019-12-15T19:22:25.433273Z","relpermalink":"/publication/pentyala-etal-2019-multi/","section":"publication","summary":"We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.","tags":["amazon","acl","multi-task","neural"],"title":"Multi-Task Networks with Universe, Group, and Task Feature Learning","type":"publication"},{"authors":["Anjishnu Kumar","Arpit Gupta","Julian Chan","Sam Tucker","Bjorn Hoffmeister","Markus Dreyer","Stanislav Peshterliev","Ankur Gandhe","Denis Filiminov","Ariya Rastrow","Christian Monson","Agnika Kumar"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"f53388c5ddfd06f49b163e07085e6c8c","permalink":"/publication/kumar-2017-just-ask/","publishdate":"2021-12-28T00:55:42.530368Z","relpermalink":"/publication/kumar-2017-just-ask/","section":"publication","summary":"This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK) a large scale Spoken Language Understanding (SLU) Software Development Kit (SDK) that enables developers to extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the infrastructure powers over 25,000 skills deployed through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability and a rapid iteration cycle for third party developers. It imposes inductive biases that allow it to learn robust SLU models from extremely small and sparse datasets and, in doing so, removes significant barriers to entry for software developers and dialogue systems researchers.","tags":["slot filling","amazon","speech","nlu"],"title":"Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding","type":"publication"},{"authors":["Xing Fan","Emilio Monti","Lambert Mathias","Markus Dreyer"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"93f94c96c49937888b94ed2faa335a70","permalink":"/publication/fan-etal-2017-transfer/","publishdate":"2019-12-15T19:22:25.430222Z","relpermalink":"/publication/fan-etal-2017-transfer/","section":"publication","summary":"The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to the target task with smaller labeled data. We see an absolute accuracy gain ranging from 1.0% to 4.4% in in our in-house data set and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.","tags":["acl","amazon","transfer learning","neural","semantic parsing"],"title":"Transfer Learning for Neural Semantic Parsing","type":"publication"},{"authors":["Anjishnu Kumar","Pavankumar Reddy Muddireddy","Markus Dreyer","Björn Hoffmeister"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bb741db2b0256e4dd2c60d6f199cf41a","permalink":"/publication/kumar-2017/","publishdate":"2019-12-15T19:22:25.432622Z","relpermalink":"/publication/kumar-2017/","section":"publication","summary":"We present a zero-shot learning approach for text classification, predicting which natural language understanding domain can handle a given utterance. Our approach can predict domains at runtime that did not exist at training time. We achieve this extensibility by learning to project utterances and domains into the same embedding space while generating each domain-specific embedding from a set of attributes that characterize the domain. Our model is a neural network trained via ranking loss. We evaluate the performance of this zero-shot approach on a subset of a virtual assistant’s third-party domains and show the effectiveness of the technique on new domains not observed during training. We compare to generative baselines and show that our approach requires less storage and performs better on new domains.","tags":["interspeech","zero-shot","nlu","amazon","neural","transfer learning"],"title":"Zero-Shot Learning Across Heterogeneous Overlapping Domains","type":"publication"},{"authors":["Markus Dreyer"],"categories":["Demo"],"content":"  div.figure { float: right; width: 50%; text-align: center; font-style: italic; font-size: smaller; text-indent: 0; border: thin silver solid; margin: 0.5em; padding: 0.5em; }  Overview Welcome to the interactive modes wheel! This simple tool is designed to:\n help you understand the modes visually, on an intuitive level, and support your compositions and improvisations practically.  A small version of the modes wheel. See the large version below or click here for a popup.  Theory. The modes wheel shows you the relationships of the different modes along the chromatic circle of 12 notes: How is the major mode (aka Ionian mode) related to the other modes, e.g., Dorian or Lydian? You can see the answer directly on the modes wheel. In addition, you can read off the notes of any mode in any key.\nYou can also see why a particular mode has its particular whole and half step pattern. For example, you see that the Phrygian mode starts with a half step interval (leading to its exotic feel) because it starts at the 3rd note of major \u0026ndash; just before major\u0026rsquo;s half step from the 3rd to the 4th note.\nThe modes wheel answers music theory questions visually, such as:\n What are the notes of C Dorian? What are the chords in B\u0026#9837; Lydian? Which mode starts with two consecutive major chords?  Composition and Improvisation. The modes wheel is also a practical tool to support your compositions and improvisations. It shows which chords and notes you can play in any key or mode. For example, it shows that a I-IV-V progression in C major is C-F-G; moving the wheel a whole step transposes it to D-G-A. Setting the wheel to D Mixolydian would change the progression to D-G-Am.\nThe modes wheel can give answers to practical questions like:\n What notes and chords do I use to compose in C Lydian? How do the chords in A Minor differ from the chords in A Dorian? I wanto to compose something around E minor \u0026ndash; what other notes and chords fit well?   Open Link in Popup \nThis is a test. Yes, it is. Another one.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"586bbbcf441dd5916709d803d0ac375a","permalink":"/post/wheel/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/wheel/","section":"post","summary":"Move the wheel to see the 7 modes of major in action!","tags":["Academic"],"title":"Modes Wheel","type":"post"},{"authors":["Faisal Ladhak","Ankur Gandhe","Markus Dreyer","Lambert Mathias","Ariya Rastrow","Björn Hoffmeister"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0ce7d3de6671c831e8da6762400f2a47","permalink":"/publication/ladhak-2016-lattice-rnn-rn/","publishdate":"2019-12-15T19:22:25.429723Z","relpermalink":"/publication/ladhak-2016-lattice-rnn-rn/","section":"publication","summary":"We present a new model called LatticeRnn, which generalizes recurrent neural networks (RNNs) to process weighted lattices as input, instead of sequences. A LatticeRnn can encode the complete structure of a lattice into a dense representation, which makes it suitable to a variety of problems, including rescoring, classifying, parsing, or translating lattices using deep neural networks (DNNs). In this paper, we use LatticeRnns for a classification task: each lattice represents the output from an automatic speech recognition (ASR) component of a spoken language understanding (SLU) system, and we classify the intent of the spoken utterance based on the lattice embedding computed by a LatticeRnn. We show that making decisions based on the full ASR output lattice, as opposed to 1-best or n-best hypotheses, makes SLU systems more robust to ASR errors. Our experiments yield improvements of 13% over a baseline RNN system trained on transcriptions and 10% over an n-best list rescoring system for intent classification.","tags":["interspeech","amazon","lattice","rnn","neural","speech"],"title":"LatticeRnn: Recurrent Neural Networks Over Lattices","type":"publication"},{"authors":["Markus Dreyer","Jonathan Graehl"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"3326e50931ec0a64aa523f4d23839497","permalink":"/publication/dreyer-graehl-2015-hyp/","publishdate":"2019-12-15T19:22:25.433986Z","relpermalink":"/publication/dreyer-graehl-2015-hyp/","section":"publication","summary":"We present hyp, an open-source toolkit for the representation, manipulation, and optimization of weighted directed hypergraphs. hyp provides compose, project, invert functionality, k-best path algorithms, the inside and outside algorithms, and more. Finite-state machines are modeled as a special case of directed hypergraphs. hyp consists of a C++ API, as well as a command line tool, and is available for download at github.com/sdl-research/hyp.","tags":["hyp","acl","dynamic programming","hypergraphs","finite state"],"title":"hyp: A Toolkit for Representing, Manipulating, and Optimizing Hypergraphs","type":"publication"},{"authors":["Markus Dreyer","Yuanzhe Dong"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"d3488d285b9d4db10cac69f3616a8b2f","permalink":"/publication/dreyer-dong-2015-apro/","publishdate":"2019-12-15T19:22:25.43463Z","relpermalink":"/publication/dreyer-dong-2015-apro/","section":"publication","summary":"We present APRO, a new method for machine translation tuning that can handle large feature sets. As opposed to other popular methods (e.g., MERT, MIRA, PRO), which involve randomness and require multiple runs to obtain a reliable result, APRO gives the same result on any run, given initial feature weights. APRO follows the pairwise ranking approach of PRO (Hopkins and May, 2011), but instead of ranking a small sampled subset of pairs from the kbest list, APRO efficiently ranks all pairs. By obviating the need for manually determined sampling settings, we obtain more reliable results. APRO converges more quickly than PRO and gives similar or better translation results.","tags":["machine translation","acl","ranking"],"title":"APRO: All-Pairs Ranking Optimization for MT Tuning","type":"publication"},{"authors":["Markus Dreyer","Daniel Marcu"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"f6763c75d648b3ea653017fc59fd07bc","permalink":"/publication/dreyer-marcu-2012-hyter/","publishdate":"2019-12-15T19:22:25.426235Z","relpermalink":"/publication/dreyer-marcu-2012-hyter/","section":"publication","summary":"It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.","tags":["emnlp","lattice","machine translation","hyter","evaluation"],"title":"HyTER: Meaning-Equivalent Semantics for Translation Evaluation","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"db5525d0094de1fa351ff07d75941755","permalink":"/publication/dreyer-eisner-2011/","publishdate":"2019-12-15T19:22:25.421057Z","relpermalink":"/publication/dreyer-eisner-2011/","section":"publication","summary":"We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming.  Given 50–100 seed paradigms, adding a 10-million-word corpus reduces prediction error for morphological inflections by up to 10%.","tags":["emnlp","morphology","monte carlo","dirichlet","belief propagation","graphical models"],"title":"Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model","type":"publication"},{"authors":["Markus Dreyer"],"categories":null,"content":"","date":1301616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301616000,"objectID":"b684e655afd4ff8e8493383be27d0368","permalink":"/publication/dreyer-2011/","publishdate":"2019-12-15T19:22:25.422447Z","relpermalink":"/publication/dreyer-2011/","section":"publication","summary":"The field of statistical natural language processing has been turning toward morphologically rich languages. These languages have vocabularies that are often orders of magnitude larger than that of English, since words may be inflected in various different ways. This leads to problems with data sparseness and calls for models that can deal with this abundance of related words—models that can learn, analyze, reduce and generate morphological inflections. But surprisingly, statistical approaches to morphology are still rare, which stands in contrast to the many recent advances of sophisticated models in parsing, grammar induction, translation and many other areas of natural language processing. This thesis presents a novel, unified statistical approach to inflectional morphology, an approach that can decode and encode the inflectional system of a language. At the center of this approach stands the notion of inflectional paradigms. These paradigms cluster the large vocabulary of a language into structured chunks; inflections of the same word, like break, broke, breaks, breaking, … , all belong in the same paradigm. And moreover, each of these inflections has an exact place within a paradigm, since each paradigm has designated slots for each possible inflection; for verbs, there is a slot for the first person singular indicative present, one for the third person plural subjunctive past and slots for all other possible forms. The main goal of this thesis is to build probability models over inflectional paradigms, and therefore to sort the large vocabulary of a morphologically rich language into structured clusters. These models can be learned with minimal supervision for any language that has inflectional morphology. As training data, some sample paradigms and a raw, unannotated text corpus can be used. The models over morphological paradigms are developed in three main chapters that start with smaller components and build up to larger ones. The first of these chapters (Chapter 2) presents novel probability models over strings and string pairs. These are applicable to lemmatization or to relate a past tense form to its associated present tense form, or for similar morphological tasks. It turns out they are general enough to tackle the popular task of transliteration very well, as well as other string-to-string tasks. The second (Chapter 3) introduces the notion of a probability model over multiple strings, which is a novel variant of Markov Random Fields. These are used to relate the many inflections in an inflectional paradigm to one another, and they use the probability models from Chapter 2 as components. A novel version of belief propagation is presented, which propagates distributions over strings through a network of connected finite-state transducers, to perform inference in morphological paradigms (or other string fields). Finally (Chapter 4), a non-parametric joint probability model over an unannotated text corpus and the morphological paradigms from Chapter 3 is presented. This model is based on a generative story for inflectional morphology that naturally incorporates common linguistic notions, such as lexemes, paradigms and inflections. Sampling algorithms are presented that perform inference over large text corpora and their implicit, hidden morphological paradigms. We show that they are able to discover the morphological paradigms that are implicit in the corpora. The model is based on finite-state operations and seamlessly handles concatenative and nonconcatenative morphology.","tags":["morphology","monte carlo","dirichlet","belief propagation","graphical models"],"title":"A Non-Parametric Model for the Discovery of Inflectional Paradigms from Plain Text Using Graphical Models over Strings","type":"publication"},{"authors":["Ariya Rastrow","Markus Dreyer","Abhinav Sethy","Sanjeev Khudanpur","Bhuvana Ramabhadran","Mark Dredze"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"aced1d049c1d17d93718ddd5d847d4ea","permalink":"/publication/conficassp-rastrow-dskrd-11/","publishdate":"2019-12-15T19:22:25.43139Z","relpermalink":"/publication/conficassp-rastrow-dskrd-11/","section":"publication","summary":"We describe a new approach for rescoring speech lattices - with long-span language models or wide-context acoustic models - that does not entail computationally intensive lattice expansion or limited rescoring of only an N-best list. We view the set of word-sequences in a lattice as a discrete space equipped with the edit-distance metric, and develop a hill climbing technique to start with, say, the 1-best hypothesis under the lattice-generating model(s) and iteratively search a local neighborhood for the highest-scoring hypothesis under the rescoring model(s); such neighborhoods are efficiently constructed via finite state techniques. We demonstrate empirically that to achieve the same reduction in error rate using a better estimated, higher order language model, our technique evaluates fewer utterance-length hypotheses than conventional N-best rescoring by two orders of magnitude. For the same number of hypotheses evaluated, our technique results in a significantly lower error rate.","tags":["speech","lattice","icassp","finite state"],"title":"Hill climbing on speech lattices: A new rescoring framework","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1249084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1249084800,"objectID":"5cf13edf8c132bf15e8c47cecc64842a","permalink":"/publication/dreyer-eisner-2009/","publishdate":"2019-12-15T19:22:25.423139Z","relpermalink":"/publication/dreyer-eisner-2009/","section":"publication","summary":"We study graphical modeling in the case of string-valued random variables. Whereas a weighted finite-state transducer can model the probabilistic relationship between two strings, we are interested in building up joint models of three or more strings. This is needed for inflectional paradigms in morphology, cognate modeling or language reconstruction, and multiple-string alignment. We propose a Markov Random Field in which each factor (potential function) is a weighted finite-state machine, typically a transducer that evaluates the relationship between just two of the strings. The full joint distribution is then a product of these factors. Though decoding is actually undecidable in general, we can still do efficient joint inference using approximate belief propagation; the necessary computations and messages are all finite-state. We demonstrate the methods by jointly predicting morphological forms.","tags":["emnlp","morphology","belief propagation","graphical models"],"title":"Graphical Models over Multiple Strings","type":"publication"},{"authors":["Paul McNamee","Mark Dredze","Adam Gerber","Nikesh Garera","Tim Finin","James Mayfield","Christine Piatko","Delip Rao","David Yarowsky","Markus Dreyer"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"ac966ed6ec6f6d03d5fcc5d017ca5b0e","permalink":"/publication/mc-namee-2009/","publishdate":"2019-12-15T19:22:25.426918Z","relpermalink":"/publication/mc-namee-2009/","section":"publication","summary":"The HLTCOE participated in the entity linking and slot filling tasks at TAC 2009. A machine learning-based approach to entity linking, operating over a wide range of feature types, yielded good performance on the entity linking task. Slot-filling based on sentence selection, application of weak patterns and exploitation of redundancy was ineffective in the slot filling task.","tags":["tac","entity","slot filling"],"title":"HLTCOE Approaches to Knowledge Base Population at TAC 2009","type":"publication"},{"authors":["Markus Dreyer","Jason R. Smith","Jason Eisner"],"categories":null,"content":"","date":1222819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222819200,"objectID":"7b3f3a69acd442072e8b28556b5c90e4","permalink":"/publication/dreyer-smith-eisner-2008/","publishdate":"2019-12-15T19:22:25.423855Z","relpermalink":"/publication/dreyer-smith-eisner-2008/","section":"publication","summary":"String-to-string transduction is a central problem in computational linguistics and natural language processing. It occurs in tasks as diverse as name transliteration, spelling correction, pronunciation modeling and inflectional morphology. We present a conditional loglinear model for string-to-string transduction, which employs overlapping features over latent alignment sequences, and which learns latent classes and latent string pair regions from incomplete training data. We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results, even when trained on small data sets. On the task of generating morphological forms, we outperform a baseline method reducing the error rate by up to 48%. On a lemmatization task, we reduce the error rates in Wicentowski (2002) by 38–92%.","tags":["morphology","log-linear","finite state","lemmatization","latent variables"],"title":"Latent-Variable Modeling of String Transductions with Finite-State Methods","type":"publication"},{"authors":["Damianos Karakos","Jason Eisner","Sanjeev Khudanpur","Markus Dreyer"],"categories":null,"content":"","date":1212278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212278400,"objectID":"e314034706499c2d068995210fac7cef","permalink":"/publication/karakos-et-al-2008/","publishdate":"2019-12-15T19:22:25.424593Z","relpermalink":"/publication/karakos-et-al-2008/","section":"publication","summary":"Given several systems' automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks.","tags":["acl","machine translation","itg","confusion network"],"title":"Machine Translation System Combination using ITG-based Alignments","type":"publication"},{"authors":["Markus Dreyer","Keith Hall","Sanjeev Khudanpur"],"categories":null,"content":"","date":1175385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1175385600,"objectID":"85fbc607e0885299b5f92994a909c4f7","permalink":"/publication/dreyer-etal-2007-comparing/","publishdate":"2019-12-15T19:22:25.427559Z","relpermalink":"/publication/dreyer-etal-2007-comparing/","section":"publication","summary":"This paper describes a new method to compare reordering constraints for Statistical Machine Translation. We investigate the best possible (oracle) BLEU score achievable under different reordering constraints. Using dynamic programming, we efficiently find a reordering that approximates the highest attainable BLEU score given a reference and a set of reordering constraints. We present an empirical evaluation of popular reordering constraints: local constraints, the IBM constraints, and the Inversion Transduction Grammar (ITG) constraints. We present results for a German-English translation task and show that reordering under the ITG constraints can improve over the baseline by more than 7.5 BLEU points.","tags":["naacl","machine translation","dynamic programming","reordering"],"title":"Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation","type":"publication"},{"authors":["M. Dreyer","I. Shafran"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"6438ca4ac574feab4c9536023c829f12","permalink":"/publication/dreyer-2007/","publishdate":"2019-12-15T19:22:25.432009Z","relpermalink":"/publication/dreyer-2007/","section":"publication","summary":"We propose novel methods for integrating prosody in syntax using generative models. By adopting a grammar whose constituents have latent annotations, the influence of prosody on syntax can be learned from data. In one method, prosody is utilized to seed the latent annotations of a grammar which is then refined using EM iterations. In an orthogonal approach, we integrate prosody into grammar more explicitly using a model that jointly observes words and associated prosody. We evaluate the two methods by parsing speech data from the Switchboard corpus. The results are compared against baseline results from a model that does not use prosody. The experiments show that prosody improves a grammar in terms of accuracy as well as the parsimonious use of parameters.","tags":["interspeech","prosody","speech","latent variables"],"title":"Exploiting prosody for PCFGs with latent annotations","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1151712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1151712000,"objectID":"665c5b751f24130443ca0bed6ac82f35","permalink":"/publication/dreyer-eisner-2006/","publishdate":"2019-12-15T19:22:25.425411Z","relpermalink":"/publication/dreyer-eisner-2006/","section":"publication","summary":"We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.","tags":["emnlp","parsing","semi-supervised","latent variables"],"title":"Better Informed Training of Latent Syntactic Features","type":"publication"},{"authors":["Markus Dreyer","David A. Smith","Noah A. Smith"],"categories":null,"content":"","date":1149120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1149120000,"objectID":"3476c4de032824b8e25cbdaa25604242","permalink":"/publication/dreyer-etal-2006-vine/","publishdate":"2019-12-15T19:22:25.42903Z","relpermalink":"/publication/dreyer-etal-2006-vine/","section":"publication","summary":"We describe our entry in the CoNLL-X shared task. The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of crosslingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data).","tags":["conll","parsing","ranking"],"title":"Vine Parsing and Minimum Risk Reranking for Speed and Precision","type":"publication"},{"authors":["A. Burbank","M. Carpuat","S. Clark","M. Dreyer","P. Fox","D. Groves","K. Hall","M. Hearne","I. D Melamed","Y. Shen"," Others"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"f01dcac7b84df73a4977102e03a3ec4c","permalink":"/publication/burbank-2005/","publishdate":"2019-12-15T19:22:25.430818Z","relpermalink":"/publication/burbank-2005/","section":"publication","summary":"Designers of SMT system have begun to experiment with tree-structured translation models. Unfortunately, SMT systems driven by such models are even more difficult to build than the already complicated WFST-based systems. The purpose of our workshop was to lower the barriers to entry into research involving such SMT systems. Our goals were inspired by the successful 1999 MT workshop, which had a similar purpose. Specifically, we wanted to follow that precedent to 1. build a publicly available toolkit for experimenting with tree-structured translation models; 2. build a tool for visualizing the predictions of such models; 3. demonstrate the feasibility of SMT with tree-structured models by running baseline experiments on large datasets; and 4. demonstrate that it is easy to retarget the toolkit to new language pairs.","tags":["parsing","machine translation"],"title":"Final report of the 2005 language engineering workshop on statistical machine translation by parsing","type":"publication"}]