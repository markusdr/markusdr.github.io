[{"authors":["admin"],"categories":null,"content":"Markus Dreyer is a Machine Learning Scientist at Amazon. He has been part of the Alexa group since 2015, leading projects in natural language understanding and question answering. He has published on transfer learning, semi-supervised learning, graphical models, semantic parsing, neural lattice parsing, and entity linking.\n","date":1549324800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Markus Dreyer is a Machine Learning Scientist at Amazon. He has been part of the Alexa group since 2015, leading projects in natural language understanding and question answering. He has published on transfer learning, semi-supervised learning, graphical models, semantic parsing, neural lattice parsing, and entity linking.","tags":null,"title":"Markus Dreyer","type":"authors"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ```  renders as\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D;  An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ```  renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers.  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Shiva Pentyala","Mengwen Liu","Markus Dreyer"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"dfc82e4090d689cc6ec6def8d2d93ead","permalink":"/publication/pentyala-etal-2019-multi/","publishdate":"2019-12-25T06:19:21.096313Z","relpermalink":"/publication/pentyala-etal-2019-multi/","section":"publication","summary":"We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.","tags":["amazon","acl","multi-task","neural"],"title":"Multi-Task Networks with Universe, Group, and Task Feature Learning","type":"publication"},{"authors":["Markus Dreyer"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":["Anjishnu Kumar","Arpit Gupta","Julian Chan","Sam Tucker","Bjorn Hoffmeister","Markus Dreyer","Stanislav Peshterliev","Ankur Gandhe","Denis Filiminov","Ariya Rastrow","Christian Monson","Agnika Kumar"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"f53388c5ddfd06f49b163e07085e6c8c","permalink":"/publication/kumar-2017-just-ask/","publishdate":"2019-12-25T06:19:21.086861Z","relpermalink":"/publication/kumar-2017-just-ask/","section":"publication","summary":"This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK) a large scale Spoken Language Understanding (SLU) Software Development Kit (SDK) that enables developers to extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the infrastructure powers over 25,000 skills deployed through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability and a rapid iteration cycle for third party developers. It imposes inductive biases that allow it to learn robust SLU models from extremely small and sparse datasets and, in doing so, removes significant barriers to entry for software developers and dialogue systems researchers.","tags":["slot filling","amazon","speech","nlu"],"title":"Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding","type":"publication"},{"authors":["Xing Fan","Emilio Monti","Lambert Mathias","Markus Dreyer"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"93f94c96c49937888b94ed2faa335a70","permalink":"/publication/fan-etal-2017-transfer/","publishdate":"2019-12-25T06:19:21.090712Z","relpermalink":"/publication/fan-etal-2017-transfer/","section":"publication","summary":"The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to the target task with smaller labeled data. We see an absolute accuracy gain ranging from 1.0% to 4.4% in in our in-house data set and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.","tags":["acl","amazon","transfer learning","neural","semantic parsing"],"title":"Transfer Learning for Neural Semantic Parsing","type":"publication"},{"authors":["Anjishnu Kumar","Pavankumar Reddy Muddireddy","Markus Dreyer","BjÃ¶rn Hoffmeister"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bb741db2b0256e4dd2c60d6f199cf41a","permalink":"/publication/kumar-2017/","publishdate":"2019-12-25T06:19:21.095372Z","relpermalink":"/publication/kumar-2017/","section":"publication","summary":"We present a zero-shot learning approach for text classification, predicting which natural language understanding domain can handle a given utterance. Our approach can predict domains at runtime that did not exist at training time. We achieve this extensibility by learning to project utterances and domains into the same embedding space while generating each domain-specific embedding from a set of attributes that characterize the domain. Our model is a neural network trained via ranking loss. We evaluate the performance of this zero-shot approach on a subset of a virtual assistantâ€™s third-party domains and show the effectiveness of the technique on new domains not observed during training. We compare to generative baselines and show that our approach requires less storage and performs better on new domains.","tags":["interspeech","zero-shot","nlu","amazon","neural","transfer learning"],"title":"Zero-Shot Learning Across Heterogeneous Overlapping Domains","type":"publication"},{"authors":["Markus Dreyer"],"categories":["Demo"],"content":"test\n Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Move the wheel to see the 7 modes of major in action!","tags":["Academic"],"title":"Modes Wheel: The Modes of Major Visualized","type":"post"},{"authors":["Faisal Ladhak","Ankur Gandhe","Markus Dreyer","Lambert Mathias","Ariya Rastrow","BjÃ¶rn Hoffmeister"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0ce7d3de6671c831e8da6762400f2a47","permalink":"/publication/ladhak-2016-lattice-rnn-rn/","publishdate":"2019-12-25T06:19:21.089578Z","relpermalink":"/publication/ladhak-2016-lattice-rnn-rn/","section":"publication","summary":"We present a new model called LatticeRnn, which generalizes recurrent neural networks (RNNs) to process weighted lattices as input, instead of sequences. A LatticeRnn can encode the complete structure of a lattice into a dense representation, which makes it suitable to a variety of problems, including rescoring, classifying, parsing, or translating lattices using deep neural networks (DNNs). In this paper, we use LatticeRnns for a classification task: each lattice represents the output from an automatic speech recognition (ASR) component of a spoken language understanding (SLU) system, and we classify the intent of the spoken utterance based on the lattice embedding computed by a LatticeRnn. We show that making decisions based on the full ASR output lattice, as opposed to 1-best or n-best hypotheses, makes SLU systems more robust to ASR errors. Our experiments yield improvements of 13% over a baseline RNN system trained on transcriptions and 10% over an n-best list rescoring system for intent classification.","tags":["interspeech","amazon","lattice","rnn","neural","speech"],"title":"LatticeRnn: Recurrent Neural Networks Over Lattices","type":"publication"},{"authors":["Markus Dreyer","Jonathan Graehl"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"3326e50931ec0a64aa523f4d23839497","permalink":"/publication/dreyer-graehl-2015-hyp/","publishdate":"2019-12-25T06:19:21.097365Z","relpermalink":"/publication/dreyer-graehl-2015-hyp/","section":"publication","summary":"We present hyp, an open-source toolkit for the representation, manipulation, and optimization of weighted directed hypergraphs. hyp provides compose, project, invert functionality, k-best path algorithms, the inside and outside algorithms, and more. Finite-state machines are modeled as a special case of directed hypergraphs. hyp consists of a C++ API, as well as a command line tool, and is available for download at github.com/sdl-research/hyp.","tags":["hyp","acl","dynamic programming","hypergraphs","finite state"],"title":"hyp: A Toolkit for Representing, Manipulating, and Optimizing Hypergraphs","type":"publication"},{"authors":["Markus Dreyer","Yuanzhe Dong"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"d3488d285b9d4db10cac69f3616a8b2f","permalink":"/publication/dreyer-dong-2015-apro/","publishdate":"2019-12-25T06:19:21.098711Z","relpermalink":"/publication/dreyer-dong-2015-apro/","section":"publication","summary":"We present APRO, a new method for machine translation tuning that can handle large feature sets. As opposed to other popular methods (e.g., MERT, MIRA, PRO), which involve randomness and require multiple runs to obtain a reliable result, APRO gives the same result on any run, given initial feature weights. APRO follows the pairwise ranking approach of PRO (Hopkins and May, 2011), but instead of ranking a small sampled subset of pairs from the kbest list, APRO efficiently ranks all pairs. By obviating the need for manually determined sampling settings, we obtain more reliable results. APRO converges more quickly than PRO and gives similar or better translation results.","tags":["machine translation","acl","ranking"],"title":"APRO: All-Pairs Ranking Optimization for MT Tuning","type":"publication"},{"authors":["Markus Dreyer","Daniel Marcu"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"f6763c75d648b3ea653017fc59fd07bc","permalink":"/publication/dreyer-marcu-2012-hyter/","publishdate":"2019-12-25T06:19:21.08311Z","relpermalink":"/publication/dreyer-marcu-2012-hyter/","section":"publication","summary":"It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.","tags":["emnlp","lattice","machine translation","hyter","evaluation"],"title":"HyTER: Meaning-Equivalent Semantics for Translation Evaluation","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"db5525d0094de1fa351ff07d75941755","permalink":"/publication/dreyer-eisner-2011/","publishdate":"2019-12-25T06:19:21.067948Z","relpermalink":"/publication/dreyer-eisner-2011/","section":"publication","summary":"We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming.  Given 50â€“100 seed paradigms, adding a 10-million-word corpus reduces prediction error for morphological inflections by up to 10%.","tags":["emnlp","morphology","monte carlo","dirichlet","belief propagation","graphical models"],"title":"Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model","type":"publication"},{"authors":["Markus Dreyer"],"categories":null,"content":"","date":1301616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301616000,"objectID":"b684e655afd4ff8e8493383be27d0368","permalink":"/publication/dreyer-2011/","publishdate":"2019-12-25T06:19:21.070689Z","relpermalink":"/publication/dreyer-2011/","section":"publication","summary":"The field of statistical natural language processing has been turning toward morphologically rich languages. These languages have vocabularies that are often orders of magnitude larger than that of English, since words may be inflected in various different ways. This leads to problems with data sparseness and calls for models that can deal with this abundance of related wordsâ€”models that can learn, analyze, reduce and generate morphological inflections. But surprisingly, statistical approaches to morphology are still rare, which stands in contrast to the many recent advances of sophisticated models in parsing, grammar induction, translation and many other areas of natural language processing. This thesis presents a novel, unified statistical approach to inflectional morphology, an approach that can decode and encode the inflectional system of a language. At the center of this approach stands the notion of inflectional paradigms. These paradigms cluster the large vocabulary of a language into structured chunks; inflections of the same word, like break, broke, breaks, breaking, â€¦ , all belong in the same paradigm. And moreover, each of these inflections has an exact place within a paradigm, since each paradigm has designated slots for each possible inflection; for verbs, there is a slot for the first person singular indicative present, one for the third person plural subjunctive past and slots for all other possible forms. The main goal of this thesis is to build probability models over inflectional paradigms, and therefore to sort the large vocabulary of a morphologically rich language into structured clusters. These models can be learned with minimal supervision for any language that has inflectional morphology. As training data, some sample paradigms and a raw, unannotated text corpus can be used. The models over morphological paradigms are developed in three main chapters that start with smaller components and build up to larger ones. The first of these chapters (Chapter 2) presents novel probability models over strings and string pairs. These are applicable to lemmatization or to relate a past tense form to its associated present tense form, or for similar morphological tasks. It turns out they are general enough to tackle the popular task of transliteration very well, as well as other string-to-string tasks. The second (Chapter 3) introduces the notion of a probability model over multiple strings, which is a novel variant of Markov Random Fields. These are used to relate the many inflections in an inflectional paradigm to one another, and they use the probability models from Chapter 2 as components. A novel version of belief propagation is presented, which propagates distributions over strings through a network of connected finite-state transducers, to perform inference in morphological paradigms (or other string fields). Finally (Chapter 4), a non-parametric joint probability model over an unannotated text corpus and the morphological paradigms from Chapter 3 is presented. This model is based on a generative story for inflectional morphology that naturally incorporates common linguistic notions, such as lexemes, paradigms and inflections. Sampling algorithms are presented that perform inference over large text corpora and their implicit, hidden morphological paradigms. We show that they are able to discover the morphological paradigms that are implicit in the corpora. The model is based on finite-state operations and seamlessly handles concatenative and nonconcatenative morphology.","tags":["morphology","monte carlo","dirichlet","belief propagation","graphical models"],"title":"A Non-Parametric Model for the Discovery of Inflectional Paradigms from Plain Text Using Graphical Models over Strings","type":"publication"},{"authors":["Ariya Rastrow","Markus Dreyer","Abhinav Sethy","Sanjeev Khudanpur","Bhuvana Ramabhadran","Mark Dredze"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"aced1d049c1d17d93718ddd5d847d4ea","permalink":"/publication/conficassp-rastrow-dskrd-11/","publishdate":"2019-12-25T06:19:21.092888Z","relpermalink":"/publication/conficassp-rastrow-dskrd-11/","section":"publication","summary":"We describe a new approach for rescoring speech lattices - with long-span language models or wide-context acoustic models - that does not entail computationally intensive lattice expansion or limited rescoring of only an N-best list. We view the set of word-sequences in a lattice as a discrete space equipped with the edit-distance metric, and develop a hill climbing technique to start with, say, the 1-best hypothesis under the lattice-generating model(s) and iteratively search a local neighborhood for the highest-scoring hypothesis under the rescoring model(s); such neighborhoods are efficiently constructed via finite state techniques. We demonstrate empirically that to achieve the same reduction in error rate using a better estimated, higher order language model, our technique evaluates fewer utterance-length hypotheses than conventional N-best rescoring by two orders of magnitude. For the same number of hypotheses evaluated, our technique results in a significantly lower error rate.","tags":["speech","lattice","icassp","finite state"],"title":"Hill climbing on speech lattices: A new rescoring framework","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1249084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1249084800,"objectID":"5cf13edf8c132bf15e8c47cecc64842a","permalink":"/publication/dreyer-eisner-2009/","publishdate":"2019-12-25T06:19:21.071744Z","relpermalink":"/publication/dreyer-eisner-2009/","section":"publication","summary":"We study graphical modeling in the case of string-valued random variables. Whereas a weighted finite-state transducer can model the probabilistic relationship between two strings, we are interested in building up joint models of three or more strings. This is needed for inflectional paradigms in morphology, cognate modeling or language reconstruction, and multiple-string alignment. We propose a Markov Random Field in which each factor (potential function) is a weighted finite-state machine, typically a transducer that evaluates the relationship between just two of the strings. The full joint distribution is then a product of these factors. Though decoding is actually undecidable in general, we can still do efficient joint inference using approximate belief propagation; the necessary computations and messages are all finite-state. We demonstrate the methods by jointly predicting morphological forms.","tags":["emnlp","morphology","belief propagation","graphical models"],"title":"Graphical Models over Multiple Strings","type":"publication"},{"authors":["Paul McNamee","Mark Dredze","Adam Gerber","Nikesh Garera","Tim Finin","James Mayfield","Christine Piatko","Delip Rao","David Yarowsky","Markus Dreyer"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"ac966ed6ec6f6d03d5fcc5d017ca5b0e","permalink":"/publication/mc-namee-2009/","publishdate":"2019-12-25T06:19:21.08431Z","relpermalink":"/publication/mc-namee-2009/","section":"publication","summary":"The HLTCOE participated in the entity linking and slot filling tasks at TAC 2009. A machine learning-based approach to entity linking, operating over a wide range of feature types, yielded good performance on the entity linking task. Slot-filling based on sentence selection, application of weak patterns and exploitation of redundancy was ineffective in the slot filling task.","tags":["tac","entity","slot filling"],"title":"HLTCOE Approaches to Knowledge Base Population at TAC 2009","type":"publication"},{"authors":["Markus Dreyer","Jason R. Smith","Jason Eisner"],"categories":null,"content":"","date":1222819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222819200,"objectID":"7b3f3a69acd442072e8b28556b5c90e4","permalink":"/publication/dreyer-smith-eisner-2008/","publishdate":"2019-12-25T06:19:21.072493Z","relpermalink":"/publication/dreyer-smith-eisner-2008/","section":"publication","summary":"String-to-string transduction is a central problem in computational linguistics and natural language processing. It occurs in tasks as diverse as name transliteration, spelling correction, pronunciation modeling and inflectional morphology. We present a conditional loglinear model for string-to-string transduction, which employs overlapping features over latent alignment sequences, and which learns latent classes and latent string pair regions from incomplete training data. We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results, even when trained on small data sets. On the task of generating morphological forms, we outperform a baseline method reducing the error rate by up to 48%. On a lemmatization task, we reduce the error rates in Wicentowski (2002) by 38â€“92%.","tags":["morphology","log-linear","finite state","lemmatization","latent variables"],"title":"Latent-Variable Modeling of String Transductions with Finite-State Methods","type":"publication"},{"authors":["Damianos Karakos","Jason Eisner","Sanjeev Khudanpur","Markus Dreyer"],"categories":null,"content":"","date":1212278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212278400,"objectID":"e314034706499c2d068995210fac7cef","permalink":"/publication/karakos-et-al-2008/","publishdate":"2019-12-25T06:19:21.080407Z","relpermalink":"/publication/karakos-et-al-2008/","section":"publication","summary":"Given several systems' automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks.","tags":["acl","machine translation","itg","confusion network"],"title":"Machine Translation System Combination using ITG-based Alignments","type":"publication"},{"authors":["Markus Dreyer","Keith Hall","Sanjeev Khudanpur"],"categories":null,"content":"","date":1175385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1175385600,"objectID":"85fbc607e0885299b5f92994a909c4f7","permalink":"/publication/dreyer-etal-2007-comparing/","publishdate":"2019-12-25T06:19:21.085667Z","relpermalink":"/publication/dreyer-etal-2007-comparing/","section":"publication","summary":"This paper describes a new method to compare reordering constraints for Statistical Machine Translation. We investigate the best possible (oracle) BLEU score achievable under different reordering constraints. Using dynamic programming, we efficiently find a reordering that approximates the highest attainable BLEU score given a reference and a set of reordering constraints. We present an empirical evaluation of popular reordering constraints: local constraints, the IBM constraints, and the Inversion Transduction Grammar (ITG) constraints. We present results for a German-English translation task and show that reordering under the ITG constraints can improve over the baseline by more than 7.5 BLEU points.","tags":["naacl","machine translation","dynamic programming","reordering"],"title":"Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation","type":"publication"},{"authors":["M. Dreyer","I. Shafran"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"6438ca4ac574feab4c9536023c829f12","permalink":"/publication/dreyer-2007/","publishdate":"2019-12-25T06:19:21.094068Z","relpermalink":"/publication/dreyer-2007/","section":"publication","summary":"We propose novel methods for integrating prosody in syntax using generative models. By adopting a grammar whose constituents have latent annotations, the influence of prosody on syntax can be learned from data. In one method, prosody is utilized to seed the latent annotations of a grammar which is then refined using EM iterations. In an orthogonal approach, we integrate prosody into grammar more explicitly using a model that jointly observes words and associated prosody. We evaluate the two methods by parsing speech data from the Switchboard corpus. The results are compared against baseline results from a model that does not use prosody. The experiments show that prosody improves a grammar in terms of accuracy as well as the parsimonious use of parameters.","tags":["interspeech","prosody","speech","latent variables"],"title":"Exploiting prosody for PCFGs with latent annotations","type":"publication"},{"authors":["Markus Dreyer","Jason Eisner"],"categories":null,"content":"","date":1151712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1151712000,"objectID":"665c5b751f24130443ca0bed6ac82f35","permalink":"/publication/dreyer-eisner-2006/","publishdate":"2019-12-25T06:19:21.082012Z","relpermalink":"/publication/dreyer-eisner-2006/","section":"publication","summary":"We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each nodeâ€™s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.","tags":["emnlp","parsing","semi-supervised","latent variables"],"title":"Better Informed Training of Latent Syntactic Features","type":"publication"},{"authors":["Markus Dreyer","David A. Smith","Noah A. Smith"],"categories":null,"content":"","date":1149120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1149120000,"objectID":"3476c4de032824b8e25cbdaa25604242","permalink":"/publication/dreyer-etal-2006-vine/","publishdate":"2019-12-25T06:19:21.088003Z","relpermalink":"/publication/dreyer-etal-2006-vine/","section":"publication","summary":"We describe our entry in the CoNLL-X shared task. The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of crosslingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data).","tags":["conll","parsing","ranking"],"title":"Vine Parsing and Minimum Risk Reranking for Speed and Precision","type":"publication"},{"authors":["A. Burbank","M. Carpuat","S. Clark","M. Dreyer","P. Fox","D. Groves","K. Hall","M. Hearne","I. D Melamed","Y. Shen"," Others"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"f01dcac7b84df73a4977102e03a3ec4c","permalink":"/publication/burbank-2005/","publishdate":"2019-12-25T06:19:21.091843Z","relpermalink":"/publication/burbank-2005/","section":"publication","summary":"Designers of SMT system have begun to experiment with tree-structured translation models. Unfortunately, SMT systems driven by such models are even more difficult to build than the already complicated WFST-based systems. The purpose of our workshop was to lower the barriers to entry into research involving such SMT systems. Our goals were inspired by the successful 1999 MT workshop, which had a similar purpose. Specifically, we wanted to follow that precedent to 1. build a publicly available toolkit for experimenting with tree-structured translation models; 2. build a tool for visualizing the predictions of such models; 3. demonstrate the feasibility of SMT with tree-structured models by running baseline experiments on large datasets; and 4. demonstrate that it is easy to retarget the toolkit to new language pairs.","tags":["parsing","machine translation"],"title":"Final report of the 2005 language engineering workshop on statistical machine translation by parsing","type":"publication"}]