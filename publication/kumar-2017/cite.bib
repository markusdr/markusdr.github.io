@inproceedings{Kumar2017,
 abstract = {We present a zero-shot learning approach for text classification, predicting which natural language understanding domain can handle a given utterance. Our approach can predict domains at runtime that did not exist at training time. We achieve this extensibility by learning to project utterances and domains into the same embedding space while generating each domain-specific embedding from a set of attributes that characterize the domain. Our model is a neural network trained via ranking loss. We evaluate the performance of this zero-shot approach on a subset of a virtual assistant’s third-party domains and show the effectiveness of the technique on new domains not observed during training. We compare to generative baselines and show that our approach requires less storage and performs better on new domains.},
 author = {Anjishnu Kumar and Pavankumar Reddy Muddireddy and Markus Dreyer and Björn Hoffmeister},
 booktitle = {Proceedings of Interspeech},
 doi = {10.21437/Interspeech.2017-516},
 keywords = {interspeech,zero-shot,nlu,amazon,neural,transfer learning},
 pages = {2914--2918},
 title = {Zero-Shot Learning Across Heterogeneous Overlapping Domains},
 url = {http://dx.doi.org/10.21437/Interspeech.2017-516},
 year = {2017}
}

